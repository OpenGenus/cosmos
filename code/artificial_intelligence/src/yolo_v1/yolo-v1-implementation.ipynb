{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORTING PACKAGES**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\n#for dataset loading\nimport os\nimport pandas as pd\nfrom PIL import Image\n\n#for training\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom tqdm import tqdm #for progress bar\nfrom torch.utils.data import DataLoader\n\n#for visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom collections import Counter\n\n#For checkpoint\nimport time\nimport sys\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:12:58.694312Z","iopub.execute_input":"2021-10-25T17:12:58.694669Z","iopub.status.idle":"2021-10-25T17:12:58.700624Z","shell.execute_reply.started":"2021-10-25T17:12:58.694632Z","shell.execute_reply":"2021-10-25T17:12:58.699683Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **DEFINING CONSTANTS**","metadata":{}},{"cell_type":"code","source":"seed = 123\ntorch.manual_seed(seed)\n\n# Hyperparameters \nLEARNING_RATE = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\nBATCH_SIZE = 16 # 64 in original paper but setting it to 16 due to low computation power\nWEIGHT_DECAY = 0\nEPOCHS = 1000\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = False #True, if you wanna use existing trained model at LOAD_MODEL_FILE path, else set to False\nLOAD_MODEL_FILE = \"/kaggle/working/best_model.pth.tar\"\nIS_TRAIN = True #True, if you wanna train model else set to False for testing\n\n#dataset paths\n\"\"\"\nChange root directory according to your file directory. \nFor kaggle keep as default. \nFor google colab, mount desired google drive, and set path to 'gdrive/My Drive/<your-dataset-file>'\nFor local PC, set value according to your file directory after downloading the dataset from https://www.kaggle.com/dataset/734b7bcb7ef13a045cbdd007a3c19874c2586ed0b02b4afc86126e89d00af8d2.\n\"\"\"\nROOT_DIR = '/kaggle/input/pascalvoc-yolo/'  \nIMG_DIR = ROOT_DIR + \"images\"\nLABEL_DIR = ROOT_DIR + \"labels\"\nFULL_DATASET_FILEPATH = ROOT_DIR + \"train.csv\"\nEIGHT_EXAMPLE_DATASET_FILEPATH = ROOT_DIR + \"8examples.csv\"\nHUNDRED_EXAMPLE_DATASET_FILEPATH = ROOT_DIR + \"100examples.csv\"\nTEST_DATASET_FILEPATH = ROOT_DIR + \"test.csv\"\n\n#class labels\nCLASS_LABELS = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"table\", \"dog\", \"horse\", \"motorbike\", \"person\", \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\"]\nassert len(CLASS_LABELS) == 20, \"Some classes are missing from class labels!\"\n\"Class labels are correct!\"","metadata":{"execution":{"iopub.status.busy":"2021-10-25T18:19:05.168345Z","iopub.execute_input":"2021-10-25T18:19:05.168627Z","iopub.status.idle":"2021-10-25T18:19:05.182824Z","shell.execute_reply.started":"2021-10-25T18:19:05.168597Z","shell.execute_reply":"2021-10-25T18:19:05.181934Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# **ARCHITECTURE CONFIG**","metadata":{}},{"cell_type":"code","source":"#refer YOLO V1 paper for the architecture\n#architecture excluding fully connected layer output\narchitecture_config = [\n    #Tuple: (kernel_size, num_filters, stride, padding)\n    (7, 64, 2, 3),\n    #String: stands for maxpool layer\n    \"M\",\n    (3, 192, 1, 1),\n    \"M\",\n    (1,128, 1, 0),\n    (3,256, 1, 1),\n    (1,256, 1, 0),\n    (3,512, 1, 1),\n    \"M\",\n    #List: [#Tuple,.., int represents no of repeats]\n    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n    (1, 512, 1, 0),\n    (3, 1024, 1, 1),\n    \"M\",\n    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n    (3, 1024, 1, 1),\n    (3, 1024, 2, 1),\n    (3, 1024, 1, 1),\n    (3, 1024, 1, 1)\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:04.879059Z","iopub.execute_input":"2021-10-25T17:13:04.879354Z","iopub.status.idle":"2021-10-25T17:13:04.887056Z","shell.execute_reply.started":"2021-10-25T17:13:04.879321Z","shell.execute_reply":"2021-10-25T17:13:04.886367Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **YOLO NETWORK DEFINATION**","metadata":{}},{"cell_type":"code","source":"#represents each convolution layer with a conv layer, batch norm and a relu\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(CNNBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias = False, **kwargs)\n        self.batchnorm = nn.BatchNorm2d(out_channels) #wasn't used in YOLO V1, but this one gives better results\n        self.leakyrelu = nn.LeakyReLU(0.1)\n        \n    def forward(self, x):\n        return self.leakyrelu(self.batchnorm(self.conv(x)))       ","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:05.999073Z","iopub.execute_input":"2021-10-25T17:13:05.999669Z","iopub.status.idle":"2021-10-25T17:13:06.005350Z","shell.execute_reply.started":"2021-10-25T17:13:05.999629Z","shell.execute_reply":"2021-10-25T17:13:06.004501Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Yolov1(nn.Module):\n    def __init__(self, in_channels= 3, **kwargs):\n        super(Yolov1, self).__init__()\n        self.architecture = architecture_config\n        self.in_channels = in_channels\n        #the entire convolution section is referred as darknet, so darknet + fully connected layer = YOLO\n        self.darknet = self._create_conv_layers(self.architecture)\n        self.fcs = self._create_fcs(**kwargs)\n    \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim = 1)) # starting from 1 because 0th dimension contains no of examples\n    \n    def _create_conv_layers(self, architecture):\n        layers = [] # add all layers to thsi list\n        in_channels = self.in_channels\n        \n        for x in architecture:\n            if(type(x) == tuple):\n               # Tuple structure for ref: (kernel_size(0), num_filters(1), stride(2), padding(3))\n                layers += [CNNBlock(\n                    in_channels,\n                    x[1], \n                    kernel_size = x[0], \n                    stride = x[2], \n                    padding = x[3]\n                )]\n                in_channels = x[1]\n                \n            elif (type(x) == str):\n                layers += [nn.MaxPool2d(kernel_size = 2, stride = 2)]\n                \n            elif (type(x) == list):\n                #List structure for ref: [#Tuple,.., int represents no of repeats]\n                num_repeats = x[2] #int\n                num_conv = len(x) - 1\n                for _ in range(num_repeats):\n                    for i in range(num_conv):                        \n                        layers += [CNNBlock(\n                            in_channels, \n                            x[i][1], \n                            kernel_size = x[i][0], \n                            stride = x[i][2], \n                            padding = x[i][3], \n                        )]\n                        in_channels = x[i][1]\n                        \n        return nn.Sequential(*layers)\n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        S, B, C = split_size, num_boxes, num_classes\n        return nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1024 * S * S, 496), #originally its 4096, instead of 496 \n            nn.Dropout(0.0), #originally 0.5\n            nn.LeakyReLU(0.1),\n            nn.Linear(496, S * S * (C + B * 5)), #originally its 4096, instead of 496, (S , S, 30) C+B*5 = 30  \n        )                ","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:06.972619Z","iopub.execute_input":"2021-10-25T17:13:06.973068Z","iopub.status.idle":"2021-10-25T17:13:06.987365Z","shell.execute_reply.started":"2021-10-25T17:13:06.973032Z","shell.execute_reply":"2021-10-25T17:13:06.986622Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **TEST CASE FOR TESTING YOLO MODEL**","metadata":{}},{"cell_type":"code","source":"#DO NOT EDIT THIS, USE THIS FOR CONFIRMING THE YOLO MODEL\ndef test(S = 7, B = 2, C = 20):\n    model = Yolov1(split_size = S, num_boxes = B, num_classes = C)\n    x = torch.randn((2, 3, 448, 448))\n    assert model(x).shape == torch.Tensor(2, 1470).shape, \"Something wrong with the model!\"\n    print('Model is correct!')\ntest()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:08.578905Z","iopub.execute_input":"2021-10-25T17:13:08.579325Z","iopub.status.idle":"2021-10-25T17:13:10.626877Z","shell.execute_reply.started":"2021-10-25T17:13:08.579290Z","shell.execute_reply":"2021-10-25T17:13:10.626130Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY METHODS**","metadata":{}},{"cell_type":"code","source":"#Calculates intersection over union(iou)\ndef intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):  \n\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    # .clamp(0) is for the case when they do not intersect\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n\n#For Non Max Suppression given bboxes\ndef non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms\n\n#for calculating mean average precision\ndef mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)\n\n#Plots bounding boxes on the image\ndef plot_image(image, boxes):\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # box[0] is x midpoint, box[2] is width\n    # box[1] is y midpoint, box[3] is height\n\n    # Create a Rectangle potch\n    for box in boxes:\n        class_label = CLASS_LABELS[int(box[0])]\n        score = box[1]\n        box = box[2:]\n        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\" #to ensure there are only 4 elements in the box dimensions\n        upper_left_x = box[0] - box[2] / 2\n        upper_left_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        label = \"%s (%.3f)\" % (class_label, score)\n        plt.text(upper_left_x * width, upper_left_y * height, label, color='white', bbox = dict(facecolor='red', alpha=0.5, edgecolor='red'))\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=\"cuda\",\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n#for output of YOLO \ndef convert_cellboxes(predictions, S=7):\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, 7, 7, 30)\n    bboxes1 = predictions[..., 21:25]\n    bboxes2 = predictions[..., 26:30]\n    scores = torch.cat(\n        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n#convert cellboxes to box for whole image\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\n#for saving checkpoint of image\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n#for loading the same checkpoint\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:10.628713Z","iopub.execute_input":"2021-10-25T17:13:10.629133Z","iopub.status.idle":"2021-10-25T17:13:10.675020Z","shell.execute_reply.started":"2021-10-25T17:13:10.629095Z","shell.execute_reply":"2021-10-25T17:13:10.674316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **LOSS FUNCTION**","metadata":{}},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    def __init__(self, S=7, B=2, C=20):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n\n        \"\"\"\n        S is split size of image (in paper 7),\n        B is number of boxes (in paper 2),\n        C is number of classes (in paper and VOC dataset is 20),\n        \"\"\"\n        self.S = S\n        self.B = B\n        self.C = C\n\n        # These are from Yolo paper, signifying how much we should\n        # pay loss for no object (noobj) and the box coordinates (coord)\n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n\n    def forward(self, predictions, target):\n        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes with target bbox\n        # 0-19 will be class probabilities, 20 will be class score, 21-25 will be bounding values and box conv1, 26-30 will be bounding values and box conv2\n        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        # Note that bestbox will be indices of 0, 1 for which bbox was best\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n\n        # ======================== #\n        #   FOR BOX COORDINATES    #\n        # ======================== #\n\n        # Set boxes with no object in them to 0. We only take out one of the two \n        # predictions, which is the one with highest Iou calculated previously.\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., 26:30]\n                + (1 - bestbox) * predictions[..., 21:25]\n            )\n        )\n\n        box_targets = exists_box * target[..., 21:25]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # ==================== #\n        #   FOR OBJECT LOSS    #\n        # ==================== #\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., 20:21]),\n        )\n\n        # ======================= #\n        #   FOR NO OBJECT LOSS    #\n        # ======================= #\n        #(N, S, S, 1) > (N, S*S)\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n        )\n\n        # ================== #\n        #   FOR CLASS LOSS   #\n        # ================== #\n        #(N, S, S, 20) > ( N*N*S , 20)\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:11.700682Z","iopub.execute_input":"2021-10-25T17:13:11.703535Z","iopub.status.idle":"2021-10-25T17:13:11.735600Z","shell.execute_reply.started":"2021-10-25T17:13:11.703486Z","shell.execute_reply":"2021-10-25T17:13:11.734194Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **DATASET**","metadata":{}},{"cell_type":"code","source":"#Common for Kaggle, Colab and Local, send paths for directories and train csv accordingly\nclass VOCDataset(torch.utils.data.Dataset):\n    def __init__(\n        self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None,\n    ):\n        self.annotations = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.transform = transform\n        self.S = S\n        self.B = B\n        self.C = C\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n        boxes = []\n        with open(label_path) as f:\n            for label in f.readlines():\n                class_label, x, y, width, height = [\n                    float(x) if float(x) != int(float(x)) else int(x)\n                    for x in label.replace(\"\\n\", \"\").split()\n                ]\n\n                boxes.append([class_label, x, y, width, height])\n\n        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        boxes = torch.tensor(boxes)\n\n        if self.transform:\n            # image = self.transform(image)\n            image, boxes = self.transform(image, boxes)\n\n        # Convert To Cells\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n\n            #Calculating the width and height of cell of bounding box,relative to the cell\n            \n            width_cell, height_cell = (\n                width * self.S,\n                height * self.S,\n            )\n\n            # If no object already found for specific cell i,j\n            if label_matrix[i, j, 20] == 0:\n                # Set that there exists an object\n                label_matrix[i, j, 20] = 1\n\n                # Box coordinates\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n\n                label_matrix[i, j, 21:25] = box_coordinates\n\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:12.534572Z","iopub.execute_input":"2021-10-25T17:13:12.535036Z","iopub.status.idle":"2021-10-25T17:13:12.550829Z","shell.execute_reply.started":"2021-10-25T17:13:12.534994Z","shell.execute_reply":"2021-10-25T17:13:12.549573Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#user-defined class for transforms\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),]) #normalization can also be added","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:12.919487Z","iopub.execute_input":"2021-10-25T17:13:12.919721Z","iopub.status.idle":"2021-10-25T17:13:12.925149Z","shell.execute_reply.started":"2021-10-25T17:13:12.919695Z","shell.execute_reply":"2021-10-25T17:13:12.924225Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **TRAINING MODEL**","metadata":{}},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True) #for progress bar\n    mean_loss = []\n\n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # update progress bar\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Mean loss : {sum(mean_loss)/len(mean_loss)}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:13:13.763720Z","iopub.execute_input":"2021-10-25T17:13:13.764205Z","iopub.status.idle":"2021-10-25T17:13:13.770323Z","shell.execute_reply.started":"2021-10-25T17:13:13.764166Z","shell.execute_reply":"2021-10-25T17:13:13.769328Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **WHERE IT ALL COMES TOGETHER!**","metadata":{}},{"cell_type":"code","source":"model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\noptimizer = optim.Adam(\n    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\nloss_fn = YoloLoss()\n\nif LOAD_MODEL:\n    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\ntrain_dataset = VOCDataset(\n    HUNDRED_EXAMPLE_DATASET_FILEPATH,\n    transform=transform,\n    img_dir=IMG_DIR,\n    label_dir=LABEL_DIR,\n)\n\n\ntest_dataset = VOCDataset(\n    TEST_DATASET_FILEPATH, transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR\n)\n\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    shuffle=True,\n    drop_last=True,\n)\nprint(\"Training set loaded...\")\n\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    shuffle=True,\n    drop_last=True,\n)\nprint(\"Testing set loaded...\")\n\nbest_acc = 0.0\nif IS_TRAIN:\n    start_training = datetime.datetime.now()\n    for epoch in range(EPOCHS):\n        print(f\"Running epoch : {epoch + 1}/{EPOCHS}\")\n        \n        pred_boxes_train, target_boxes_train = get_bboxes(\n            train_loader, model, iou_threshold=0.5, threshold=0.4\n        )\n\n        mean_avg_prec_train = mean_average_precision(\n            pred_boxes_train, target_boxes_train, iou_threshold=0.5, box_format=\"midpoint\"\n        )\n\n        print(f\"Train mAP: {mean_avg_prec_train}\")\n    \n        if mean_avg_prec > best_acc:\n            best_acc = mean_avg_prec\n    \n#         if mean_avg_prec >0.9 and mean_avg_prec == best_acc:\n#             checkpoint = {\n#                 \"state_dict\": model.state_dict(),\n#                 \"optimizer\": optimizer.state_dict(),\n#             }\n#             save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n#             time.sleep(10)\n    \n        train_fn(train_loader, model, optimizer, loss_fn)\n\n    print(f\"Total training time {(datetime.datetime.now()-start_training).seconds/60} minutes.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T18:19:16.023684Z","iopub.execute_input":"2021-10-25T18:19:16.023940Z","iopub.status.idle":"2021-10-25T19:12:14.010246Z","shell.execute_reply.started":"2021-10-25T18:19:16.023912Z","shell.execute_reply":"2021-10-25T19:12:14.009434Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#for testing\n# load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\nfor x, y in train_loader:\n    x = x.to(DEVICE)\n    for idx in range(2):\n        bboxes_pred = cellboxes_to_boxes(model(x))\n        bboxes_pred = non_max_suppression(bboxes_pred[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes_pred)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:12:14.012418Z","iopub.execute_input":"2021-10-25T19:12:14.017437Z","iopub.status.idle":"2021-10-25T19:12:19.714810Z","shell.execute_reply.started":"2021-10-25T19:12:14.017394Z","shell.execute_reply":"2021-10-25T19:12:19.714029Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}